{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9e2fb6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython \u001b[36m3.10.17\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!uv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "cdc8eeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: /Users/ElliotPhua/miniconda3\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!source .venv/bin/activate\n",
    "!uv pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c092287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "corpus = \"\" # corpus\n",
    "\n",
    "def preprocess_corpus(corpus: str) -> tuple[dict[str, int], dict[int, str], np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        corpus (str)\n",
    "    Returns:\n",
    "        dict[str, int]: ind_dict[k=word, v=index]\n",
    "        dict[int, str]: str_dict[k=index, v=word]\n",
    "        np.ndarray: an ndarray of word indices corresponding to the input corpus (ground truth)\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", corpus)[:20000]\n",
    "    ind_dict = {} # k=word, v=ind\n",
    "    str_dict = {} # k=ind, v=word\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word not in ind_dict:\n",
    "            ind_dict[word] = count\n",
    "            str_dict[count] = word\n",
    "            count += 1\n",
    "    \n",
    "    data_ind = np.array([ind_dict[w] for w in tokens])\n",
    "    \n",
    "    return ind_dict, str_dict, data_ind, count\n",
    "\n",
    "def make_sequences(data_ind: np.ndarray, seq_len:int=10) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X = np.array([data_ind[i:i+seq_len] for i in range(len(data_ind) - seq_len)], dtype=int)\n",
    "    Y = np.array([data_ind[j:j+seq_len] for j in range(1, len(data_ind) - seq_len + 1)], dtype=int)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "42a280f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network\n",
    "import numpy as np\n",
    "\n",
    "def get_h_t(x_t:int, ind_dict: dict[str, int], prev_h:np.ndarray, w_e:np.ndarray, w_hh:np.ndarray, w_xh:np.ndarray, b_h:np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    h_t <- f(x_t, h_t-1)\n",
    "    Get current state memory, dependent on current word embedding e_t and state memory at previous time step prev_h, and using the weight embedding w_hh.\n",
    "    Args:\n",
    "        x_t (int): The current target word's index\n",
    "        ind_dict (dict[str, int]): indices dictionary, k=word, v=corresponding row index, length |v|\n",
    "        prev_h (np.ndarray): The state memory at previous time step t-1, with shape H x 1\n",
    "        w_e (np.ndarray): word embedding matrix, shape |v| x d\n",
    "        w_hh (np.ndarray): weight embedding for state memory, shape H x H\n",
    "        w_xh (np.ndarray): weight embedding to transform target word embedding into space of state memory embedding, shape H x d\n",
    "        b_h (np.ndarray): bias with shape H x 1\n",
    "    Returns:\n",
    "        np.ndarray: the current state memory h_t, shape H x 1\n",
    "    '''\n",
    "    one_hot = np.zeros((len(ind_dict), 1)) # shape |v| x 1\n",
    "    one_hot[x_t] = 1 # one-hot encoding for current word index\n",
    "    e_t = w_e.T@one_hot # Get the word embedding for the current word index, shape d x 1\n",
    "    h_t = np.tanh(w_xh@e_t + w_hh@prev_h + b_h) # tanh prevents exploding gradients and gives hidden state non-linearity\n",
    "    return h_t\n",
    "\n",
    "def predict_o_t(h_t:np.ndarray, w_hy:np.ndarray, b_y:np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Get the output raw score for all candidate output words\n",
    "    Args:\n",
    "        h_t (np.ndarray): the current state memory embedding, shape H x 1\n",
    "        w_hy (np.ndarray): the output score embedding, shape |v| x H\n",
    "        b_y (np.ndarray): the bias for output score, shape |v| x 1\n",
    "    Returns:\n",
    "        np.ndarray: the raw score vector for all candidate words, shape |v| x 1\n",
    "    '''\n",
    "    return w_hy@h_t + b_y\n",
    "\n",
    "def softmax(o_t: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    softmax(o_i) = exp(o_i - C)/sum[j in T](exp(o_j - C)), numerical stability\n",
    "    Args:\n",
    "        o_t (np.ndarray): the output vector of raw scores at time step t, shape |v| x 1\n",
    "    Returns:\n",
    "        np.ndarray: the probability vector for all candidate output word, shape |v| x 1\n",
    "    \"\"\"\n",
    "    exp_z = np.exp(o_t - np.max(o_t))\n",
    "    return exp_z/np.sum(exp_z)\n",
    "    \n",
    "def get_most_likely_output_ind(p_t:np.ndarray) -> int:\n",
    "    index_of_max_p = np.argmax(p_t)\n",
    "    return int(index_of_max_p)\n",
    "\n",
    "def loss_at_time_step(p_t: np.ndarray, i: int, v_len: int) -> float:\n",
    "    \"\"\"\n",
    "    Cross entropy loss.\n",
    "    Args:\n",
    "        p_t (np.ndarray): the output probability vector for all candidate words at time step t, shape: |v| x 1\n",
    "        i (int): index of target word\n",
    "        v_len (int): vocab length\n",
    "    Returns:\n",
    "        float: the loss at time step t\n",
    "    \"\"\"\n",
    "    y_true = np.zeros((v_len, 1))\n",
    "    y_true[i] = 1\n",
    "    loss_t = -np.log(y_true.T@p_t + 1e-12)\n",
    "    return loss_t[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "59f14b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init weights manually\n",
    "class Weights():\n",
    "    def __init__(self, ind_dict: dict[str, int], H:int=128, d:int=64) -> None:\n",
    "        self.H = H # hidden state size\n",
    "        self.d = d # word embedding sizes\n",
    "        self.v = len(ind_dict) # vocab length\n",
    "        self.c = 0.01\n",
    "        self.w_e = np.random.randn(self.v, self.d) * self.c\n",
    "        self.w_xh = np.random.randn(self.H, self.d) * self.c\n",
    "        self.b_h = np.zeros((self.H, 1)) # biases are offsets, no symmetry issue; can start with neutral offset. weights are init randomly so that they can learn diff parts of the data distribution\n",
    "        self.w_hh = np.random.randn(self.H, self.H) * self.c\n",
    "        self.w_hy = np.random.randn(self.v, self.H) * self.c\n",
    "        self.b_y = np.zeros((self.v, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "62001647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "\n",
    "def forward_prop(x_seq: np.ndarray, y_seq: np.ndarray, h_prev: np.ndarray, ind_dict: dict[str, int], model:Weights) -> tuple:\n",
    "    w_e, w_xh, w_hh, w_hy, b_h, b_y = model.w_e, model.w_xh, model.w_hh, model.w_hy, model.b_h, model.b_y\n",
    "    \n",
    "    total_loss, correct_pred = 0, 0\n",
    "    h_cache, o_cache, p_cache = {}, {}, {}\n",
    "    h_cache[-1] = np.copy(h_prev)\n",
    "    \n",
    "    for t in range(len(x_seq)):\n",
    "        h_cache[t] = get_h_t(x_seq[t], ind_dict, h_cache[t-1], w_e, w_hh, w_xh, b_h)\n",
    "        o_cache[t] = predict_o_t(h_cache[t], w_hy, b_y)\n",
    "        p_cache[t] = softmax(o_cache[t])\n",
    "        pred_ind = get_most_likely_output_ind(p_cache[t])\n",
    "        correct_pred += pred_ind == y_seq[t]\n",
    "        l_t = loss_at_time_step(p_cache[t], y_seq[t], len(ind_dict))\n",
    "        total_loss += l_t\n",
    "    \n",
    "    accuracy_score = float(correct_pred / y_seq.size)\n",
    "    cache = (x_seq, h_cache, p_cache)\n",
    "    return total_loss, accuracy_score, h_cache[len(x_seq)-1], cache    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8b5406f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation through time (BPTT)\n",
    "# we want to train w_e, w_xh, b_x, w_hh, w_hy, b_y\n",
    "\n",
    "def backprop_tt(y_seq: np.ndarray, model:Weights, cache:tuple, learning_rate:float=0.1):\n",
    "    w_e, w_xh, w_hh, w_hy, b_h, b_y = (\n",
    "        model.w_e,\n",
    "        model.w_xh,\n",
    "        model.w_hh,\n",
    "        model.w_hy,\n",
    "        model.b_h,\n",
    "        model.b_y,\n",
    "    )\n",
    "\n",
    "    dw_e = np.zeros_like(w_e)\n",
    "    dw_xh = np.zeros_like(w_xh)\n",
    "    dw_hh = np.zeros_like(w_hh)\n",
    "    db_h = np.zeros_like(b_h)\n",
    "    dw_hy = np.zeros_like(w_hy)\n",
    "    db_y = np.zeros_like(b_y) # shape |v| x 1\n",
    "\n",
    "    x_cache, h_cache, p_cache = cache\n",
    "\n",
    "    dh_next = np.zeros_like(h_cache[0])\n",
    "\n",
    "    for t in reversed(range(len(y_seq))):\n",
    "        x_t = x_cache[t]\n",
    "        y_true = y_seq[t]\n",
    "\n",
    "        # dL/dy_t = p_t - one-hot y_t\n",
    "        d_o = p_cache[t].copy() \n",
    "        d_o[y_true] -= 1 # derivative of softmax + cross entropy\n",
    "\n",
    "        dw_hy += d_o @ h_cache[t].T \n",
    "        db_y += d_o\n",
    "\n",
    "        dh_t = w_hy.T @ d_o + dh_next\n",
    "        dh_raw = (1-h_cache[t]**2) * dh_t # tanh derivative\n",
    "\n",
    "        db_h += dh_raw\n",
    "        e_t = w_e[x_t].reshape(1, -1)\n",
    "        dw_xh += dh_raw @ e_t\n",
    "        dw_hh += dh_raw @ h_cache[t-1].T\n",
    "        dw_e[x_t] += (w_xh.T @ dh_raw).flatten()\n",
    "        dh_next = w_hh.T @ dh_raw\n",
    "\n",
    "    # Gradient clipping to prevent exploding gradient\n",
    "    for grad in [dw_xh, dw_hh, dw_hy, db_h, db_y, dw_e]:\n",
    "        np.clip(grad, -5, 5, out=grad)\n",
    "\n",
    "    # update weights sgd\n",
    "    model.w_e -= learning_rate * dw_e\n",
    "    model.w_xh -= learning_rate * dw_xh\n",
    "    model.w_hh -= learning_rate * dw_hh\n",
    "    model.w_hy -= learning_rate * dw_hy\n",
    "    model.b_h -= learning_rate * db_h\n",
    "    model.b_y -= learning_rate * db_y\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "86b25201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19995, 5) (19995, 5)\n",
      "4290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fp = \"data/text8\"\n",
    "\n",
    "with open(fp, 'r') as f:\n",
    "    corpus = f.read().lower()\n",
    "\n",
    "ind_dict, str_dict, data_indices, vocab_len = preprocess_corpus(corpus)\n",
    "\n",
    "X, Y = make_sequences(data_indices, seq_len=5)\n",
    "print(X.shape, Y.shape)\n",
    "print(len(ind_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4f2552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if shuffle = False --> preserve order, if shuffle = True --> faster convergence if huge dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_smol, y_train_smol = X_train[:500], y_train[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "80e66435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4290, 64) (128, 64) (128, 128) (4290, 128)\n"
     ]
    }
   ],
   "source": [
    "# initialise input embedding, w_xh, w_hh, w_hy\n",
    "model = Weights(ind_dict)\n",
    "print(model.w_e.shape, model.w_xh.shape, model.w_hh.shape, model.w_hy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9e4b9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual training\n",
    "\n",
    "def RNN_train(X_train: np.ndarray, y_train: np.ndarray, ind_dict: dict[str, int], model: Weights, hidden_size:int=128, d:int=64, epochs:int=100, learning_rate=0.3e-1) -> Weights:\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_epoch_loss = 0\n",
    "        h_prev = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        total_correct  = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for i in range(len(X_train)):\n",
    "            x_seq = X_train[i]\n",
    "            y_seq = y_train[i]\n",
    "            \n",
    "            total_loss, accuracy_score, h_prev, cache = forward_prop(x_seq, y_seq, h_prev, ind_dict, model)\n",
    "            \n",
    "            total_epoch_loss += total_loss\n",
    "            \n",
    "            total_correct += accuracy_score * y_seq.size\n",
    "            total_words += y_seq.size\n",
    "            model = backprop_tt(y_seq, model, cache, learning_rate)  # Fixed order of arguments\n",
    "        \n",
    "        avg_loss = total_epoch_loss / len(X_train)\n",
    "        epoch_accuracy = total_correct / total_words\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Average Loss: {avg_loss}, Accuracy Score: {epoch_accuracy}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "92343235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Average Loss: [30.38594098], Accuracy Score: 0.0508\n",
      "Epoch 2/100: Average Loss: [28.30600715], Accuracy Score: 0.058\n",
      "Epoch 3/100: Average Loss: [26.51425503], Accuracy Score: 0.0584\n",
      "Epoch 4/100: Average Loss: [25.4367435], Accuracy Score: 0.064\n",
      "Epoch 5/100: Average Loss: [24.47576544], Accuracy Score: 0.0584\n",
      "Epoch 6/100: Average Loss: [23.84344435], Accuracy Score: 0.0716\n",
      "Epoch 7/100: Average Loss: [23.30761517], Accuracy Score: 0.0652\n",
      "Epoch 8/100: Average Loss: [22.7599109], Accuracy Score: 0.0796\n",
      "Epoch 9/100: Average Loss: [22.49833956], Accuracy Score: 0.0792\n",
      "Epoch 10/100: Average Loss: [22.24451067], Accuracy Score: 0.088\n",
      "Epoch 11/100: Average Loss: [22.04581514], Accuracy Score: 0.086\n",
      "Epoch 12/100: Average Loss: [21.49996201], Accuracy Score: 0.1064\n",
      "Epoch 13/100: Average Loss: [21.41550184], Accuracy Score: 0.0988\n",
      "Epoch 14/100: Average Loss: [21.0578449], Accuracy Score: 0.116\n",
      "Epoch 15/100: Average Loss: [21.08194796], Accuracy Score: 0.1156\n",
      "Epoch 16/100: Average Loss: [20.18291706], Accuracy Score: 0.1244\n",
      "Epoch 17/100: Average Loss: [19.97035319], Accuracy Score: 0.1348\n",
      "Epoch 18/100: Average Loss: [20.24776912], Accuracy Score: 0.132\n",
      "Epoch 19/100: Average Loss: [19.98967468], Accuracy Score: 0.1476\n",
      "Epoch 20/100: Average Loss: [20.29874857], Accuracy Score: 0.1236\n",
      "Epoch 21/100: Average Loss: [19.70493579], Accuracy Score: 0.1356\n",
      "Epoch 22/100: Average Loss: [19.55265163], Accuracy Score: 0.1348\n",
      "Epoch 23/100: Average Loss: [19.29681259], Accuracy Score: 0.1512\n",
      "Epoch 24/100: Average Loss: [19.31189821], Accuracy Score: 0.1544\n",
      "Epoch 25/100: Average Loss: [19.86222618], Accuracy Score: 0.144\n",
      "Epoch 26/100: Average Loss: [18.63377456], Accuracy Score: 0.1688\n",
      "Epoch 27/100: Average Loss: [18.24483024], Accuracy Score: 0.19\n",
      "Epoch 28/100: Average Loss: [18.39956574], Accuracy Score: 0.1844\n",
      "Epoch 29/100: Average Loss: [19.62755377], Accuracy Score: 0.1456\n",
      "Epoch 30/100: Average Loss: [19.77981971], Accuracy Score: 0.156\n",
      "Epoch 31/100: Average Loss: [19.4281947], Accuracy Score: 0.152\n",
      "Epoch 32/100: Average Loss: [19.03892783], Accuracy Score: 0.16\n",
      "Epoch 33/100: Average Loss: [18.75410195], Accuracy Score: 0.1628\n",
      "Epoch 34/100: Average Loss: [19.2460226], Accuracy Score: 0.1556\n",
      "Epoch 35/100: Average Loss: [19.30381422], Accuracy Score: 0.1528\n",
      "Epoch 36/100: Average Loss: [19.30608079], Accuracy Score: 0.1616\n",
      "Epoch 37/100: Average Loss: [19.28931487], Accuracy Score: 0.1564\n",
      "Epoch 38/100: Average Loss: [19.13913963], Accuracy Score: 0.1528\n",
      "Epoch 39/100: Average Loss: [19.73600199], Accuracy Score: 0.1468\n",
      "Epoch 40/100: Average Loss: [19.61715255], Accuracy Score: 0.1524\n",
      "Epoch 41/100: Average Loss: [19.32339094], Accuracy Score: 0.156\n",
      "Epoch 42/100: Average Loss: [19.27246248], Accuracy Score: 0.1588\n",
      "Epoch 43/100: Average Loss: [19.04225642], Accuracy Score: 0.1692\n",
      "Epoch 44/100: Average Loss: [19.08784951], Accuracy Score: 0.19\n",
      "Epoch 45/100: Average Loss: [18.80287025], Accuracy Score: 0.1736\n",
      "Epoch 46/100: Average Loss: [18.79256389], Accuracy Score: 0.1792\n",
      "Epoch 47/100: Average Loss: [19.01236655], Accuracy Score: 0.1808\n",
      "Epoch 48/100: Average Loss: [19.19738206], Accuracy Score: 0.172\n",
      "Epoch 49/100: Average Loss: [19.25644894], Accuracy Score: 0.1556\n",
      "Epoch 50/100: Average Loss: [19.48920936], Accuracy Score: 0.166\n",
      "Epoch 51/100: Average Loss: [19.34634788], Accuracy Score: 0.1596\n",
      "Epoch 52/100: Average Loss: [19.09696199], Accuracy Score: 0.1808\n",
      "Epoch 53/100: Average Loss: [19.91252923], Accuracy Score: 0.1312\n",
      "Epoch 54/100: Average Loss: [19.97037358], Accuracy Score: 0.1628\n",
      "Epoch 55/100: Average Loss: [19.50574643], Accuracy Score: 0.1508\n",
      "Epoch 56/100: Average Loss: [19.75047126], Accuracy Score: 0.1584\n",
      "Epoch 57/100: Average Loss: [19.36953351], Accuracy Score: 0.1616\n",
      "Epoch 58/100: Average Loss: [20.01178193], Accuracy Score: 0.15\n",
      "Epoch 59/100: Average Loss: [20.08124102], Accuracy Score: 0.1456\n",
      "Epoch 60/100: Average Loss: [19.92166272], Accuracy Score: 0.1496\n",
      "Epoch 61/100: Average Loss: [19.41557246], Accuracy Score: 0.1604\n",
      "Epoch 62/100: Average Loss: [19.65638361], Accuracy Score: 0.1592\n",
      "Epoch 63/100: Average Loss: [19.78465673], Accuracy Score: 0.1548\n",
      "Epoch 64/100: Average Loss: [19.86901762], Accuracy Score: 0.154\n",
      "Epoch 65/100: Average Loss: [19.25772752], Accuracy Score: 0.176\n",
      "Epoch 66/100: Average Loss: [19.25185784], Accuracy Score: 0.1684\n",
      "Epoch 67/100: Average Loss: [20.12925466], Accuracy Score: 0.188\n",
      "Epoch 68/100: Average Loss: [19.76367837], Accuracy Score: 0.1776\n",
      "Epoch 69/100: Average Loss: [19.07373891], Accuracy Score: 0.1904\n",
      "Epoch 70/100: Average Loss: [18.98516139], Accuracy Score: 0.1912\n",
      "Epoch 71/100: Average Loss: [18.84088113], Accuracy Score: 0.1904\n",
      "Epoch 72/100: Average Loss: [19.10206198], Accuracy Score: 0.172\n",
      "Epoch 73/100: Average Loss: [18.69952025], Accuracy Score: 0.1844\n",
      "Epoch 74/100: Average Loss: [17.58547529], Accuracy Score: 0.2208\n",
      "Epoch 75/100: Average Loss: [17.38430965], Accuracy Score: 0.2148\n",
      "Epoch 76/100: Average Loss: [17.19181917], Accuracy Score: 0.2144\n",
      "Epoch 77/100: Average Loss: [16.33985499], Accuracy Score: 0.2516\n",
      "Epoch 78/100: Average Loss: [16.82110403], Accuracy Score: 0.2416\n",
      "Epoch 79/100: Average Loss: [16.61262854], Accuracy Score: 0.2484\n",
      "Epoch 80/100: Average Loss: [16.53701359], Accuracy Score: 0.246\n",
      "Epoch 81/100: Average Loss: [16.86333593], Accuracy Score: 0.2308\n",
      "Epoch 82/100: Average Loss: [16.92078164], Accuracy Score: 0.2312\n",
      "Epoch 83/100: Average Loss: [16.63837034], Accuracy Score: 0.2396\n",
      "Epoch 84/100: Average Loss: [16.51668609], Accuracy Score: 0.25\n",
      "Epoch 85/100: Average Loss: [15.59280178], Accuracy Score: 0.266\n",
      "Epoch 86/100: Average Loss: [15.469322], Accuracy Score: 0.2688\n",
      "Epoch 87/100: Average Loss: [15.67222211], Accuracy Score: 0.2668\n",
      "Epoch 88/100: Average Loss: [15.85493007], Accuracy Score: 0.2596\n",
      "Epoch 89/100: Average Loss: [15.69647703], Accuracy Score: 0.2608\n",
      "Epoch 90/100: Average Loss: [16.04560185], Accuracy Score: 0.2572\n",
      "Epoch 91/100: Average Loss: [16.00590816], Accuracy Score: 0.2692\n",
      "Epoch 92/100: Average Loss: [16.22295864], Accuracy Score: 0.2692\n",
      "Epoch 93/100: Average Loss: [15.52855102], Accuracy Score: 0.2752\n",
      "Epoch 94/100: Average Loss: [15.54096549], Accuracy Score: 0.2832\n",
      "Epoch 95/100: Average Loss: [15.05518127], Accuracy Score: 0.2936\n",
      "Epoch 96/100: Average Loss: [14.98425282], Accuracy Score: 0.29\n",
      "Epoch 97/100: Average Loss: [15.39490866], Accuracy Score: 0.2836\n",
      "Epoch 98/100: Average Loss: [15.21619055], Accuracy Score: 0.2784\n",
      "Epoch 99/100: Average Loss: [14.54358092], Accuracy Score: 0.3036\n",
      "Epoch 100/100: Average Loss: [14.60912228], Accuracy Score: 0.3084\n"
     ]
    }
   ],
   "source": [
    "model = RNN_train(X_train_smol, y_train_smol, ind_dict, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2fd67664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_test(model: Weights, X_test: np.ndarray, y_test: np.ndarray) -> None:\n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    h_prev = np.zeros((model.H,1))\n",
    "    for i in range(len(X_test)):\n",
    "        x_seq = X_test[i]\n",
    "        y_seq = y_test[i]\n",
    "        \n",
    "        loss, _, h_prev, cache = forward_prop(x_seq, y_seq, h_prev, ind_dict, model)\n",
    "        total_loss += loss\n",
    "        _, _, p_cache = cache\n",
    "        # Count correct predictions\n",
    "        for t in range(len(x_seq)):\n",
    "            p_t = p_cache[t]\n",
    "            pred_idx = np.argmax(p_t)\n",
    "            total_correct += pred_idx == y_seq[t]\n",
    "            total_words += len(x_seq)\n",
    "    \n",
    "    avg_loss = total_loss / len(X_test)\n",
    "    accuracy_score = total_correct / total_words\n",
    "    \n",
    "    print(f\"Test loss: {avg_loss}, Test Accuracy: {accuracy_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c99ecc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: [61.39573813], Test Accuracy: 0.0059914978744686175\n"
     ]
    }
   ],
   "source": [
    "RNN_test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: /Users/ElliotPhua/miniconda3\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.9 environment at: /Users/ElliotPhua/miniconda3\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.9 environment at: /Users/ElliotPhua/miniconda3\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!uv pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Using pytorch\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else: print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5387bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
